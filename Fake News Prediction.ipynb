{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-14T14:26:41.223675Z","iopub.execute_input":"2022-02-14T14:26:41.224230Z","iopub.status.idle":"2022-02-14T14:26:41.236934Z","shell.execute_reply.started":"2022-02-14T14:26:41.224184Z","shell.execute_reply":"2022-02-14T14:26:41.235802Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:41.239386Z","iopub.execute_input":"2022-02-14T14:26:41.240026Z","iopub.status.idle":"2022-02-14T14:26:41.248458Z","shell.execute_reply.started":"2022-02-14T14:26:41.239991Z","shell.execute_reply":"2022-02-14T14:26:41.247305Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# load dataset\ndf = pd.read_csv('/kaggle/input/fake-news/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:41.249764Z","iopub.execute_input":"2022-02-14T14:26:41.250213Z","iopub.status.idle":"2022-02-14T14:26:42.823954Z","shell.execute_reply.started":"2022-02-14T14:26:41.250182Z","shell.execute_reply":"2022-02-14T14:26:42.823381Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# we will only Going to use title and author Columns for Our prediction\ndf.drop(['id','text'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.825160Z","iopub.execute_input":"2022-02-14T14:26:42.825562Z","iopub.status.idle":"2022-02-14T14:26:42.832730Z","shell.execute_reply.started":"2022-02-14T14:26:42.825521Z","shell.execute_reply":"2022-02-14T14:26:42.831625Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.835073Z","iopub.execute_input":"2022-02-14T14:26:42.835337Z","iopub.status.idle":"2022-02-14T14:26:42.853793Z","shell.execute_reply.started":"2022-02-14T14:26:42.835304Z","shell.execute_reply":"2022-02-14T14:26:42.853181Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Check for Null Values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.855085Z","iopub.execute_input":"2022-02-14T14:26:42.855819Z","iopub.status.idle":"2022-02-14T14:26:42.870514Z","shell.execute_reply.started":"2022-02-14T14:26:42.855767Z","shell.execute_reply":"2022-02-14T14:26:42.869876Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Drop Null values\ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.871789Z","iopub.execute_input":"2022-02-14T14:26:42.872558Z","iopub.status.idle":"2022-02-14T14:26:42.885089Z","shell.execute_reply.started":"2022-02-14T14:26:42.872512Z","shell.execute_reply":"2022-02-14T14:26:42.884308Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Check for Duplicated Values\ndf.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.886398Z","iopub.execute_input":"2022-02-14T14:26:42.887370Z","iopub.status.idle":"2022-02-14T14:26:42.910448Z","shell.execute_reply.started":"2022-02-14T14:26:42.887320Z","shell.execute_reply":"2022-02-14T14:26:42.909649Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Drop Duplicated Values\ndf = df.drop_duplicates(keep='first')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.911531Z","iopub.execute_input":"2022-02-14T14:26:42.911878Z","iopub.status.idle":"2022-02-14T14:26:42.929794Z","shell.execute_reply.started":"2022-02-14T14:26:42.911850Z","shell.execute_reply":"2022-02-14T14:26:42.928485Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Merge both Columns Author and Title and Create New Column Content\ndf['content'] = df['title'] + ' ' + df['author'] \ndf['content'][0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.931286Z","iopub.execute_input":"2022-02-14T14:26:42.931551Z","iopub.status.idle":"2022-02-14T14:26:42.950173Z","shell.execute_reply.started":"2022-02-14T14:26:42.931510Z","shell.execute_reply":"2022-02-14T14:26:42.949340Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"df.head()\ndf['content'][3]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.951373Z","iopub.execute_input":"2022-02-14T14:26:42.952112Z","iopub.status.idle":"2022-02-14T14:26:42.963457Z","shell.execute_reply.started":"2022-02-14T14:26:42.952075Z","shell.execute_reply":"2022-02-14T14:26:42.962450Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"df['content'][0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.964737Z","iopub.execute_input":"2022-02-14T14:26:42.965024Z","iopub.status.idle":"2022-02-14T14:26:42.975742Z","shell.execute_reply.started":"2022-02-14T14:26:42.964989Z","shell.execute_reply":"2022-02-14T14:26:42.974825Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\n1. Convert text to lowercase\n2. tokenization\n3. remove stopwords\n4. remove punctuation\n5. stemming","metadata":{}},{"cell_type":"code","source":"# import required libaries for preprocessing\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nps = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.977115Z","iopub.execute_input":"2022-02-14T14:26:42.977383Z","iopub.status.idle":"2022-02-14T14:26:42.986957Z","shell.execute_reply.started":"2022-02-14T14:26:42.977354Z","shell.execute_reply":"2022-02-14T14:26:42.986313Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:26:42.991100Z","iopub.execute_input":"2022-02-14T14:26:42.991373Z","iopub.status.idle":"2022-02-14T14:27:03.041187Z","shell.execute_reply.started":"2022-02-14T14:26:42.991344Z","shell.execute_reply":"2022-02-14T14:27:03.040292Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Function for entire text transformation\n\ndef text_preprocessing(text): \n    # Convert text into lowercase\n    text = text.lower()\n    \n    # Tokenize text into list\n    tokenize_text = nltk.word_tokenize(text)\n    \n    # remove Stopwords\n    text_without_stopwords = [i for i in tokenize_text if i not in stopwords.words('english')]\n    \n    # Remove Punctuation\n    text_without_punc = [i for i in text_without_stopwords if i not in string.punctuation]\n    \n    # fetch only alphanumeric values and apply stemming on that word\n    transformed_text = [ps.stem(i) for i in text_without_punc if i.isalnum() == True]\n    \n    return \" \".join(transformed_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:03.042593Z","iopub.execute_input":"2022-02-14T14:27:03.042879Z","iopub.status.idle":"2022-02-14T14:27:03.049287Z","shell.execute_reply.started":"2022-02-14T14:27:03.042847Z","shell.execute_reply":"2022-02-14T14:27:03.048424Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Let's Apply This Tranfomation Function on Our Content Column\ndf['transformed_content'] = df['content'].apply(text_preprocessing)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:03.050563Z","iopub.execute_input":"2022-02-14T14:27:03.051010Z","iopub.status.idle":"2022-02-14T14:27:46.567297Z","shell.execute_reply.started":"2022-02-14T14:27:03.050981Z","shell.execute_reply":"2022-02-14T14:27:46.566468Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Drop title author and old content column\ndf = df.drop(['title','author','content'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:46.568621Z","iopub.execute_input":"2022-02-14T14:27:46.568923Z","iopub.status.idle":"2022-02-14T14:27:46.580298Z","shell.execute_reply.started":"2022-02-14T14:27:46.568884Z","shell.execute_reply":"2022-02-14T14:27:46.579207Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:46.581756Z","iopub.execute_input":"2022-02-14T14:27:46.581986Z","iopub.status.idle":"2022-02-14T14:27:46.591692Z","shell.execute_reply.started":"2022-02-14T14:27:46.581958Z","shell.execute_reply":"2022-02-14T14:27:46.591014Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# let's create new column like number of characters, number of words\ndf['number_of_characters'] = df['transformed_content'].apply(lambda x:len(x))\ndf['number_of_words'] = df['transformed_content'].apply(lambda x:len(nltk.word_tokenize(x)))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:46.592770Z","iopub.execute_input":"2022-02-14T14:27:46.593437Z","iopub.status.idle":"2022-02-14T14:27:48.449527Z","shell.execute_reply.started":"2022-02-14T14:27:46.593403Z","shell.execute_reply":"2022-02-14T14:27:48.448659Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:48.450881Z","iopub.execute_input":"2022-02-14T14:27:48.451162Z","iopub.status.idle":"2022-02-14T14:27:48.462991Z","shell.execute_reply.started":"2022-02-14T14:27:48.451127Z","shell.execute_reply":"2022-02-14T14:27:48.462041Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Check Count of labels\nsns.countplot(x='label',data=df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:48.464329Z","iopub.execute_input":"2022-02-14T14:27:48.464618Z","iopub.status.idle":"2022-02-14T14:27:48.651233Z","shell.execute_reply.started":"2022-02-14T14:27:48.464584Z","shell.execute_reply":"2022-02-14T14:27:48.650274Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"### **Our Data is Balanced**","metadata":{}},{"cell_type":"code","source":"# statical info for True news\ndf[['number_of_characters','number_of_words']][df['label'] == 0].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:48.652699Z","iopub.execute_input":"2022-02-14T14:27:48.653020Z","iopub.status.idle":"2022-02-14T14:27:48.675267Z","shell.execute_reply.started":"2022-02-14T14:27:48.652974Z","shell.execute_reply":"2022-02-14T14:27:48.674113Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# statical info for fake news\ndf[['number_of_characters','number_of_words']][df['label'] == 1].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:48.676672Z","iopub.execute_input":"2022-02-14T14:27:48.677005Z","iopub.status.idle":"2022-02-14T14:27:48.697197Z","shell.execute_reply.started":"2022-02-14T14:27:48.676962Z","shell.execute_reply":"2022-02-14T14:27:48.696341Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# histplot for Fake News words and True News words\nsns.histplot(df['number_of_words'][df['label'] == 0],bins=50,binwidth=2)\nsns.histplot(df['number_of_words'][df['label'] == 1],color='red',binwidth=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:48.698583Z","iopub.execute_input":"2022-02-14T14:27:48.699047Z","iopub.status.idle":"2022-02-14T14:27:48.996708Z","shell.execute_reply.started":"2022-02-14T14:27:48.699001Z","shell.execute_reply":"2022-02-14T14:27:48.996000Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"### **Let's make wordcloud of True and Fake News Respectively. so, we can analyze that which words are often used in both type of news.**","metadata":{}},{"cell_type":"code","source":"# import wordcloud\nfrom wordcloud import WordCloud\n\n# make object of wordcloud\nwc = WordCloud(background_color='white',min_font_size=10,width=500,height=500)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:48.998203Z","iopub.execute_input":"2022-02-14T14:27:48.998486Z","iopub.status.idle":"2022-02-14T14:27:49.003077Z","shell.execute_reply.started":"2022-02-14T14:27:48.998456Z","shell.execute_reply":"2022-02-14T14:27:49.002044Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# WordCloud for True News\ntrue_news_wc = wc.generate(df[df['label'] == 0]['transformed_content'].str.cat(sep=\" \"))\nplt.figure(figsize=(8,6))\nplt.imshow(true_news_wc)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:49.004714Z","iopub.execute_input":"2022-02-14T14:27:49.005024Z","iopub.status.idle":"2022-02-14T14:27:50.698800Z","shell.execute_reply.started":"2022-02-14T14:27:49.004982Z","shell.execute_reply":"2022-02-14T14:27:50.698135Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# WordCloud for Fake news\nfake_news_wc = wc.generate(df[df['label'] == 1]['transformed_content'].str.cat(sep = \" \"))\nplt.figure(figsize=(8,6))\nplt.imshow(fake_news_wc)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:50.700104Z","iopub.execute_input":"2022-02-14T14:27:50.700518Z","iopub.status.idle":"2022-02-14T14:27:52.058800Z","shell.execute_reply.started":"2022-02-14T14:27:50.700485Z","shell.execute_reply":"2022-02-14T14:27:52.058121Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"### **We can clearly see from this wordcloud that which words are most used in true and fake news.**","metadata":{}},{"cell_type":"markdown","source":"### **Let's find specific Count of words.**","metadata":{}},{"cell_type":"code","source":"# library for Count Words\nfrom collections import Counter\n\n# create list of True News words\ntrue_news_words_list = df[df['label']==0]['transformed_content'].str.cat(sep = \" \").split()\n\n# create DataFrame of that\ntrue_news_words_df = pd.DataFrame(Counter(true_news_words_list).most_common(20))\n\n# Now Let's Plot barplot of this words\nsns.barplot(x=true_news_words_df[0],y=true_news_words_df[1])\nplt.xticks(rotation='vertical')\nplt.xlabel('Words')\nplt.ylabel('Counts')\nplt.title('True News Words Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:52.060082Z","iopub.execute_input":"2022-02-14T14:27:52.060518Z","iopub.status.idle":"2022-02-14T14:27:52.615570Z","shell.execute_reply.started":"2022-02-14T14:27:52.060469Z","shell.execute_reply":"2022-02-14T14:27:52.614633Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# create list of Fake News words\nfake_news_words_list = df[df['label']==1]['transformed_content'].str.cat(sep = \" \").split()\n\n# create DataFrame of that\nfake_news_words_df = pd.DataFrame(Counter(fake_news_words_list).most_common(20))\n\n# Now Let's Plot barplot of this words\nsns.barplot(x=fake_news_words_df[0],y=fake_news_words_df[1])\nplt.xticks(rotation='vertical')\nplt.xlabel('Words')\nplt.ylabel('Counts')\nplt.title('Fake News Words Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:52.617437Z","iopub.execute_input":"2022-02-14T14:27:52.617685Z","iopub.status.idle":"2022-02-14T14:27:52.935427Z","shell.execute_reply.started":"2022-02-14T14:27:52.617658Z","shell.execute_reply":"2022-02-14T14:27:52.934649Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Splitting**","metadata":{}},{"cell_type":"code","source":"# Let's Separate our Input and Output Columns\nX = df['transformed_content'].values\ny = df['label'].values","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:52.936720Z","iopub.execute_input":"2022-02-14T14:27:52.937155Z","iopub.status.idle":"2022-02-14T14:27:52.940845Z","shell.execute_reply.started":"2022-02-14T14:27:52.937122Z","shell.execute_reply":"2022-02-14T14:27:52.940307Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# let's do Train Test Split of Our Data\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:52.942183Z","iopub.execute_input":"2022-02-14T14:27:52.942873Z","iopub.status.idle":"2022-02-14T14:27:52.993116Z","shell.execute_reply.started":"2022-02-14T14:27:52.942838Z","shell.execute_reply":"2022-02-14T14:27:52.992323Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Now Let's Do Vectorization of Transformed Content Using Bag of Words Technique\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n# create object of CountVectorizer\ncf = CountVectorizer(max_features=5000)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:52.994651Z","iopub.execute_input":"2022-02-14T14:27:52.994991Z","iopub.status.idle":"2022-02-14T14:27:53.000566Z","shell.execute_reply.started":"2022-02-14T14:27:52.994946Z","shell.execute_reply":"2022-02-14T14:27:52.999496Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# Convert X_train and Y_train into Vevtors\nX_trf =  cf.fit_transform(X).toarray()\nX_train = cf.fit_transform(X_train).toarray()\nX_test = cf.transform(X_test).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:53.002066Z","iopub.execute_input":"2022-02-14T14:27:53.002407Z","iopub.status.idle":"2022-02-14T14:27:54.798109Z","shell.execute_reply.started":"2022-02-14T14:27:53.002363Z","shell.execute_reply":"2022-02-14T14:27:54.797143Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# **Model Building**","metadata":{}},{"cell_type":"code","source":"# import required Models \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.model_selection import GridSearchCV,ShuffleSplit,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:54.799318Z","iopub.execute_input":"2022-02-14T14:27:54.799559Z","iopub.status.idle":"2022-02-14T14:27:54.804375Z","shell.execute_reply.started":"2022-02-14T14:27:54.799529Z","shell.execute_reply":"2022-02-14T14:27:54.803155Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"def check_model(X,y):\n    algos = {\n        'lgr':{\n            'model':LogisticRegression(),\n            'params':{\n                'C':[0.1,0.01,1,0.5,2,10,20]\n            }\n        },\n        'mnb':{\n            'model':MultinomialNB(),\n            'params':{\n                \n            }\n        },\n        'bnb':{\n            'model':BernoulliNB(),\n            'params':{\n            \n            }\n        },\n        'gnb':{\n            'model':GaussianNB(),\n            'params':{\n            \n            }\n        },\n    }\n    \n    score = []\n    \n    for model_name,config in algos.items():\n        cv = ShuffleSplit(n_splits=5,test_size=0.2,random_state=42)\n        gd = GridSearchCV(estimator=config['model'],param_grid=config['params'],n_jobs=-1,cv=cv)\n        gd.fit(X,y)\n        score.append({'model_name':model_name,'acc_score':gd.best_score_,'best_params':gd.best_params_})\n        \n    return pd.DataFrame(score)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:54.805647Z","iopub.execute_input":"2022-02-14T14:27:54.805910Z","iopub.status.idle":"2022-02-14T14:27:54.822971Z","shell.execute_reply.started":"2022-02-14T14:27:54.805883Z","shell.execute_reply":"2022-02-14T14:27:54.822008Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"check_model(X_trf,y)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:27:54.825265Z","iopub.execute_input":"2022-02-14T14:27:54.827476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's Evaluate Model\ndef model_evaluation(model,X_train,X_test,y_train,y_test):\n    model.fit(X_train,y_train)\n    train_pred = model.predict(X_train)\n    test_pred = model.predict(X_test)\n    train_acc = accuracy_score(y_train,train_pred)\n    test_acc = accuracy_score(y_test,test_pred)\n    \n    train_score = pd.Series({'accuracy_score':accuracy_score(y_train,train_pred),'precision_score':precision_score(y_train,train_pred)})\n    test_score = pd.Series({'accuracy_score':accuracy_score(y_test,test_pred),'precision_score':precision_score(y_test,test_pred)})\n    scorecard = pd.concat([train_score,test_score],axis=1)\n    \n    scorecard.columns = ['Train_data','Test_data']\n    \n    return scorecard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(LogisticRegression(C=10),X_train,X_test,y_train,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(MultinomialNB(),X_train,X_test,y_train,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(BernoulliNB(),X_train,X_test,y_train,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(GaussianNB(),X_train,X_test,y_train,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Logistic Regression is Giving us Best result out of all the model. So, we are going to use Logistic Regression here**","metadata":{}},{"cell_type":"code","source":"# Build Final Model\nlg = LogisticRegression(C=10)\nlg.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1 = X_test[0]\nlg.predict(p1.reshape(1,-1))[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_news(text):\n    cleaned_text = text_preprocessing(text)\n    ps = pd.Series(cleaned_text)\n    final_text = cf.transform(ps).toarray()\n    pred = lg.predict(final_text.reshape(1,-1))[0]\n    \n    if pred == 0:\n        print(\"It's a True News\")\n    \n    else:\n        print(\"It's a Fake News\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_news(\"15 Civilians Killed In Single US Airstrike Have Been Identified Jessica Purkiss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}